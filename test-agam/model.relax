@tvm.script.ir_module
class Module:
    @T.prim_func
    def transpose(rxplaceholder: T.Buffer[(T.int64(4), T.int64(2)), "float32"], T_transpose: T.Buffer[(T.int64(2), T.int64(4)), "float32"]):
        # function attr dict
        T.func_attr({"tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1 in T.grid(T.int64(2), T.int64(4)):
            with T.block("T_transpose"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(rxplaceholder[ax1, ax0])
                T.writes(T_transpose[ax0, ax1])
                T_transpose[ax0, ax1] = rxplaceholder[ax1, ax0]
    
    @T.prim_func
    def expand_dims(rxplaceholder: T.Buffer[T.int64(2), "float32"], T_expand_dims: T.Buffer[(T.int64(1), T.int64(2)), "float32"]):
        # function attr dict
        T.func_attr({"tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1 in T.grid(T.int64(1), T.int64(2)):
            with T.block("T_expand_dims"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(rxplaceholder[ax1])
                T.writes(T_expand_dims[ax0, ax1])
                T_expand_dims[ax0, ax1] = rxplaceholder[ax1]
    
    @T.prim_func
    def relu(rxplaceholder: T.Buffer[T.int64(4), "float32"], T_relu: T.Buffer[T.int64(4), "float32"]):
        # function attr dict
        T.func_attr({"tir.noalias": True})
        # body
        # with T.block("root")
        for i0 in T.serial(T.int64(4)):
            with T.block("T_relu"):
                ax0 = T.axis.spatial(T.int64(4), i0)
                T.reads(rxplaceholder[ax0])
                T.writes(T_relu[ax0])
                T_relu[ax0] = T.max(rxplaceholder[ax0], T.float32(0))
    
    @T.prim_func
    def add(rxplaceholder: T.Buffer[T.int64(4), "float32"], rxplaceholder_1: T.Buffer[T.int64(4), "float32"], T_add: T.Buffer[T.int64(4), "float32"]):
        # function attr dict
        T.func_attr({"tir.noalias": True})
        # body
        # with T.block("root")
        for i0 in T.serial(T.int64(4)):
            with T.block("T_add"):
                ax0 = T.axis.spatial(T.int64(4), i0)
                T.reads(rxplaceholder[ax0], rxplaceholder_1[ax0])
                T.writes(T_add[ax0])
                T_add[ax0] = rxplaceholder[ax0] + rxplaceholder_1[ax0]
    
    @T.prim_func
    def matmul(rxplaceholder: T.Buffer[(T.int64(1), T.int64(2)), "float32"], rxplaceholder_1: T.Buffer[(T.int64(2), T.int64(4)), "float32"], T_matmul_NN: T.Buffer[(T.int64(1), T.int64(4)), "float32"]):
        # function attr dict
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2 in T.grid(T.int64(1), T.int64(4), T.int64(2)):
            with T.block("T_matmul_NN"):
                i, j, k = T.axis.remap("SSR", [i0, i1, i2])
                T.reads(rxplaceholder[i, k], rxplaceholder_1[k, j])
                T.writes(T_matmul_NN[i, j])
                with T.init():
                    T_matmul_NN[i, j] = T.float32(0)
                T_matmul_NN[i, j] = T_matmul_NN[i, j] + rxplaceholder[i, k] * rxplaceholder_1[k, j]
    
    @T.prim_func
    def squeeze(rxplaceholder: T.Buffer[(T.int64(1), T.int64(4)), "float32"], T_squeeze: T.Buffer[T.int64(4), "float32"]):
        # function attr dict
        T.func_attr({"tir.noalias": True})
        # body
        # with T.block("root")
        for i0 in T.serial(T.int64(4)):
            with T.block("T_squeeze"):
                ax0 = T.axis.spatial(T.int64(4), i0)
                T.reads(rxplaceholder[T.int64(0), ax0])
                T.writes(T_squeeze[ax0])
                T_squeeze[ax0] = rxplaceholder[T.int64(0), ax0]
    
    @R.function
    def main(input: R.Tensor((2,), dtype="float32"), linear_weight: R.Tensor((4, 2), dtype="float32"), linear_bias: R.Tensor((4,), dtype="float32")) -> R.Tensor(None, dtype="float32", ndim=1):
        # function attr dict
        R.func_attr({"global_symbol": "main"})
        # block 0
        with R.dataflow():
            lv = R.call_tir(expand_dims, (input,), (1, 2), dtype="float32")
            lv1 = R.call_tir(transpose, (linear_weight,), (2, 4), dtype="float32")
            lv2 = R.call_tir(matmul, (lv, lv1), (1, 4), dtype="float32")
            lv3 = R.call_tir(squeeze, (lv2,), (4,), dtype="float32")
            lv4 = R.call_tir(add, (lv3, linear_bias), (4,), dtype="float32")
            lv5 = R.call_tir(relu, (lv4,), (4,), dtype="float32")
            gv: R.Tensor((4,), dtype="float32") = lv5
            R.output(gv)
        return gv
        
