@add = primfn(var_rxplaceholder: handle, var_rxplaceholder_1: handle, var_T_add: handle) -> ()
  attr = {"global_symbol": "add", "tir.noalias": True}
  buffers = {rxplaceholder: Buffer(rxplaceholder_2: Pointer(global float32), float32, [4i64], []),
             rxplaceholder_1: Buffer(rxplaceholder_3: Pointer(global float32), float32, [4i64], []),
             T_add: Buffer(T_add_1: Pointer(global float32), float32, [4i64], [])}
  buffer_map = {var_rxplaceholder: rxplaceholder, var_rxplaceholder_1: rxplaceholder_1, var_T_add: T_add} {
  block([], "root") {
    tir.reads([])
    tir.writes([])
    for (i0: int64, 0i64, 4i64) {
      block([4i64], "T_add") as [ax0] {
        bind(ax0, i0)
        tir.reads([rxplaceholder[ax0], rxplaceholder_1[ax0]])
        tir.writes([T_add[ax0]])
        T_add[ax0] = (rxplaceholder[ax0] + rxplaceholder_1[ax0])
    }
}

@expand_dims = primfn(var_rxplaceholder_2: handle, var_T_expand_dims: handle) -> ()
  attr = {"global_symbol": "expand_dims", "tir.noalias": True}
  buffers = {rxplaceholder_4: Buffer(rxplaceholder_5: Pointer(global float32), float32, [2i64], []),
             T_expand_dims: Buffer(T_expand_dims_1: Pointer(global float32), float32, [1i64, 2i64], [])}
  buffer_map = {var_rxplaceholder_2: rxplaceholder_4, var_T_expand_dims: T_expand_dims} {
  block([], "root") {
    tir.reads([])
    tir.writes([])
    for (i0_1: int64, 0i64, 1i64) {
      for (i1: int64, 0i64, 2i64) {
        block([1i64, 2i64], "T_expand_dims") as [ax0_1, ax1] {
          bind(ax0_1, i0_1)
          bind(ax1, i1)
          tir.reads([rxplaceholder_4[ax1]])
          tir.writes([T_expand_dims[ax0_1, ax1]])
          T_expand_dims[ax0_1, ax1] = rxplaceholder_4[ax1]
      }
    }
}

@R.function
def main(input: R.Tensor((2,), dtype="float32"), linear_weight: R.Tensor((4, 2), dtype="float32"), linear_bias: R.Tensor((4,), dtype="float32")) -> R.Tensor(None, dtype="float32", ndim=1):
    # function attr dict
    R.func_attr({"global_symbol": "main"})
    # block 0
    with R.dataflow():
        lv = R.call_tir(expand_dims, (input,), (1, 2), dtype="float32")
        lv1 = R.call_tir(transpose, (linear_weight,), (2, 4), dtype="float32")
        lv2 = R.call_tir(matmul, (lv, lv1), (1, 4), dtype="float32")
        lv3 = R.call_tir(squeeze, (lv2,), (4,), dtype="float32")
        lv4 = R.call_tir(add, (lv3, linear_bias), (4,), dtype="float32")
        lv5 = R.call_tir(relu, (lv4,), (4,), dtype="float32")
        gv: R.Tensor((4,), dtype="float32") = lv5
        R.output(gv)
    return gv
    

@matmul = primfn(var_rxplaceholder_3: handle, var_rxplaceholder_4: handle, var_T_matmul_NN: handle) -> ()
  attr = {"global_symbol": "matmul", "tir.noalias": True, "layout_free_buffers": [1]}
  buffers = {rxplaceholder_6: Buffer(rxplaceholder_8: Pointer(global float32), float32, [1i64, 2i64], []),
             rxplaceholder_7: Buffer(rxplaceholder_9: Pointer(global float32), float32, [2i64, 4i64], []),
             T_matmul_NN: Buffer(T_matmul_NN_1: Pointer(global float32), float32, [1i64, 4i64], [])}
  buffer_map = {var_rxplaceholder_3: rxplaceholder_6, var_rxplaceholder_4: rxplaceholder_7, var_T_matmul_NN: T_matmul_NN} {
  block([], "root") {
    tir.reads([])
    tir.writes([])
    for (i0_2: int64, 0i64, 1i64) {
      for (i1_1: int64, 0i64, 4i64) {
        for (i2: int64, 0i64, 2i64) {
          block([1i64, 4i64, tir.reduce_axis(0i64, 2i64)], "T_matmul_NN") as [i, j, k] {
            bind(i, i0_2)
            bind(j, i1_1)
            bind(k, i2)
            tir.reads([rxplaceholder_6[i, k], rxplaceholder_7[k, j]])
            tir.writes([T_matmul_NN[i, j]])
            with init() {
              T_matmul_NN[i, j] = 0f32
            }
            T_matmul_NN[i, j] = (T_matmul_NN[i, j] + (rxplaceholder_6[i, k]*rxplaceholder_7[k, j]))
        }
      }
    }
}

@relu = primfn(var_rxplaceholder_5: handle, var_T_relu: handle) -> ()
  attr = {"global_symbol": "relu", "tir.noalias": True}
  buffers = {rxplaceholder_10: Buffer(rxplaceholder_11: Pointer(global float32), float32, [4i64], []),
             T_relu: Buffer(T_relu_1: Pointer(global float32), float32, [4i64], [])}
  buffer_map = {var_rxplaceholder_5: rxplaceholder_10, var_T_relu: T_relu} {
  block([], "root") {
    tir.reads([])
    tir.writes([])
    for (i0_3: int64, 0i64, 4i64) {
      block([4i64], "T_relu") as [ax0_2] {
        bind(ax0_2, i0_3)
        tir.reads([rxplaceholder_10[ax0_2]])
        tir.writes([T_relu[ax0_2]])
        T_relu[ax0_2] = max(rxplaceholder_10[ax0_2], 0f32)
    }
}

@squeeze = primfn(var_rxplaceholder_6: handle, var_T_squeeze: handle) -> ()
  attr = {"global_symbol": "squeeze", "tir.noalias": True}
  buffers = {rxplaceholder_12: Buffer(rxplaceholder_13: Pointer(global float32), float32, [1i64, 4i64], []),
             T_squeeze: Buffer(T_squeeze_1: Pointer(global float32), float32, [4i64], [])}
  buffer_map = {var_rxplaceholder_6: rxplaceholder_12, var_T_squeeze: T_squeeze} {
  block([], "root") {
    tir.reads([])
    tir.writes([])
    for (i0_4: int64, 0i64, 4i64) {
      block([4i64], "T_squeeze") as [ax0_3] {
        bind(ax0_3, i0_4)
        tir.reads([rxplaceholder_12[0i64, ax0_3]])
        tir.writes([T_squeeze[ax0_3]])
        T_squeeze[ax0_3] = rxplaceholder_12[0i64, ax0_3]
    }
}

@transpose = primfn(var_rxplaceholder_7: handle, var_T_transpose: handle) -> ()
  attr = {"global_symbol": "transpose", "tir.noalias": True}
  buffers = {rxplaceholder_14: Buffer(rxplaceholder_15: Pointer(global float32), float32, [4i64, 2i64], []),
             T_transpose: Buffer(T_transpose_1: Pointer(global float32), float32, [2i64, 4i64], [])}
  buffer_map = {var_rxplaceholder_7: rxplaceholder_14, var_T_transpose: T_transpose} {
  block([], "root") {
    tir.reads([])
    tir.writes([])
    for (i0_5: int64, 0i64, 2i64) {
      for (i1_2: int64, 0i64, 4i64) {
        block([2i64, 4i64], "T_transpose") as [ax0_4, ax1_1] {
          bind(ax0_4, i0_5)
          bind(ax1_1, i1_2)
          tir.reads([rxplaceholder_14[ax1_1, ax0_4]])
          tir.writes([T_transpose[ax0_4, ax1_1]])
          T_transpose[ax0_4, ax1_1] = rxplaceholder_14[ax1_1, ax0_4]
      }
    }
}


